Randomizing 100% of the dataset
Train set size 60000
Validation set size 0
Test set size 10000
Epoch [1], Loss: 2.3072
Learning rate: 0.000995
Epoch [2], Loss: 2.2989
Learning rate: 0.000990
Epoch [3], Loss: 2.2934
Learning rate: 0.000985
Epoch [4], Loss: 2.2839
Learning rate: 0.000980
Epoch [5], Loss: 2.2683
Learning rate: 0.000975
Epoch [6], Loss: 2.2468
Learning rate: 0.000970
Epoch [7], Loss: 2.2143
Learning rate: 0.000966
Epoch [8], Loss: 2.1715
Learning rate: 0.000961
Epoch [9], Loss: 2.1170
Learning rate: 0.000956
Epoch [10], Loss: 2.0502
Learning rate: 0.000951
Epoch [11], Loss: 1.9751
Learning rate: 0.000946
Epoch [12], Loss: 1.8991
Learning rate: 0.000942
Epoch [13], Loss: 1.8097
Learning rate: 0.000937
Epoch [14], Loss: 1.7217
Learning rate: 0.000932
Epoch [15], Loss: 1.6337
Learning rate: 0.000928
Epoch [16], Loss: 1.5444
Learning rate: 0.000923
Epoch [17], Loss: 1.4618
Learning rate: 0.000918
Epoch [18], Loss: 1.3745
Learning rate: 0.000914
Epoch [19], Loss: 1.2991
Learning rate: 0.000909
Epoch [20], Loss: 1.2178
Learning rate: 0.000905
Epoch [21], Loss: 1.1383
Learning rate: 0.000900
Epoch [22], Loss: 1.0645
Learning rate: 0.000896
Epoch [23], Loss: 0.9981
Learning rate: 0.000891
Epoch [24], Loss: 0.9337
Learning rate: 0.000887
Epoch [25], Loss: 0.8756
Learning rate: 0.000882
Epoch [26], Loss: 0.8133
Learning rate: 0.000878
Epoch [27], Loss: 0.7659
Learning rate: 0.000873
Epoch [28], Loss: 0.7139
Learning rate: 0.000869
Epoch [29], Loss: 0.6627
Learning rate: 0.000865
Epoch [30], Loss: 0.6220
Learning rate: 0.000860
Epoch [31], Loss: 0.5784
Learning rate: 0.000856
Epoch [32], Loss: 0.5425
Learning rate: 0.000852
Epoch [33], Loss: 0.5070
Learning rate: 0.000848
Epoch [34], Loss: 0.4695
Learning rate: 0.000843
Epoch [35], Loss: 0.4416
Learning rate: 0.000839
Epoch [36], Loss: 0.4156
Learning rate: 0.000835
Epoch [37], Loss: 0.3848
Learning rate: 0.000831
Epoch [38], Loss: 0.3571
Learning rate: 0.000827
Epoch [39], Loss: 0.3330
Learning rate: 0.000822
Epoch [40], Loss: 0.3159
Learning rate: 0.000818
Epoch [41], Loss: 0.2982
Learning rate: 0.000814
Epoch [42], Loss: 0.2806
Learning rate: 0.000810
Epoch [43], Loss: 0.2652
Learning rate: 0.000806
Epoch [44], Loss: 0.2477
Learning rate: 0.000802
Epoch [45], Loss: 0.2335
Learning rate: 0.000798
Epoch [46], Loss: 0.2197
Learning rate: 0.000794
Epoch [47], Loss: 0.2039
Learning rate: 0.000790
Epoch [48], Loss: 0.1950
Learning rate: 0.000786
Epoch [49], Loss: 0.1961
Learning rate: 0.000782
Epoch [50], Loss: 0.1989
Learning rate: 0.000778
Epoch [51], Loss: 0.1967
Learning rate: 0.000774
Epoch [52], Loss: 0.1864
Learning rate: 0.000771
Epoch [53], Loss: 0.1651
Learning rate: 0.000767
Epoch [54], Loss: 0.1492
Learning rate: 0.000763
Epoch [55], Loss: 0.1417
Learning rate: 0.000759
Epoch [56], Loss: 0.1310
Learning rate: 0.000755
Epoch [57], Loss: 0.1246
Learning rate: 0.000751
Epoch [58], Loss: 0.1181
Learning rate: 0.000748
Epoch [59], Loss: 0.1137
Learning rate: 0.000744
Epoch [60], Loss: 0.1087
Learning rate: 0.000740
Epoch [61], Loss: 0.1039
Learning rate: 0.000737
Epoch [62], Loss: 0.1000
Learning rate: 0.000733
Epoch [63], Loss: 0.0951
Learning rate: 0.000729
Epoch [64], Loss: 0.0906
Learning rate: 0.000726
Epoch [65], Loss: 0.0872
Learning rate: 0.000722
Epoch [66], Loss: 0.0841
Learning rate: 0.000718
Epoch [67], Loss: 0.0881
Learning rate: 0.000715
Epoch [68], Loss: 0.4091
Learning rate: 0.000711
Epoch [69], Loss: 0.2947
Learning rate: 0.000708
Epoch [70], Loss: 0.1122
Learning rate: 0.000704
Epoch [71], Loss: 0.0767
Learning rate: 0.000701
Epoch [72], Loss: 0.0676
Learning rate: 0.000697
Epoch [73], Loss: 0.0638
Learning rate: 0.000694
Epoch [74], Loss: 0.0601
Learning rate: 0.000690
Epoch [75], Loss: 0.0580
Learning rate: 0.000687
Epoch [76], Loss: 0.0567
Learning rate: 0.000683
Epoch [77], Loss: 0.0535
Learning rate: 0.000680
Epoch [78], Loss: 0.0529
Learning rate: 0.000676
Epoch [79], Loss: 0.0520
Learning rate: 0.000673
Epoch [80], Loss: 0.0490
Learning rate: 0.000670
Epoch [81], Loss: 0.0487
Learning rate: 0.000666
Epoch [82], Loss: 0.0460
Learning rate: 0.000663
Epoch [83], Loss: 0.0452
Learning rate: 0.000660
Epoch [84], Loss: 0.0428
Learning rate: 0.000656
Epoch [85], Loss: 0.0415
Learning rate: 0.000653
Epoch [86], Loss: 0.0426
Learning rate: 0.000650
Epoch [87], Loss: 0.0401
Learning rate: 0.000647
Epoch [88], Loss: 0.0355
Learning rate: 0.000643
Epoch [89], Loss: 0.0338
Learning rate: 0.000640
Epoch [90], Loss: 0.0336
Learning rate: 0.000637
Epoch [91], Loss: 0.0322
Learning rate: 0.000634
Epoch [92], Loss: 0.0293
Learning rate: 0.000631
Epoch [93], Loss: 0.0932
Learning rate: 0.000627
Epoch [94], Loss: 0.4466
Learning rate: 0.000624
Epoch [95], Loss: 0.1136
Learning rate: 0.000621
Epoch [96], Loss: 0.0426
Learning rate: 0.000618
Epoch [97], Loss: 0.0285
Learning rate: 0.000615
Epoch [98], Loss: 0.0256
Learning rate: 0.000612
Epoch [99], Loss: 0.0243
Learning rate: 0.000609
Epoch [100], Loss: 0.0226
Learning rate: 0.000606
Epoch [101], Loss: 0.0219
Learning rate: 0.000603
Epoch [102], Loss: 0.0211
Learning rate: 0.000600
Randomizing 100% of the dataset
Train set size 60000
Validation set size 0
Test set size 10000
Epoch [1], Loss: 2.9370
Learning rate: 0.000995
Epoch [2], Loss: 2.3024
Learning rate: 0.000990
Epoch [3], Loss: 2.3021
Learning rate: 0.000985
Epoch [4], Loss: 2.3019
Learning rate: 0.000980
Epoch [5], Loss: 2.3018
Learning rate: 0.000975
Epoch [6], Loss: 2.3015
Learning rate: 0.000970
Epoch [7], Loss: 2.3014
Learning rate: 0.000966
Epoch [8], Loss: 2.3011
Learning rate: 0.000961
Epoch [9], Loss: 2.3006
Learning rate: 0.000956
Epoch [10], Loss: 2.3004
Learning rate: 0.000951
Epoch [11], Loss: 2.3001
Learning rate: 0.000946
Epoch [12], Loss: 2.2994
Learning rate: 0.000942
Epoch [13], Loss: 2.2988
Learning rate: 0.000937
Epoch [14], Loss: 2.2982
Learning rate: 0.000932
Epoch [15], Loss: 2.2976
Learning rate: 0.000928
Epoch [16], Loss: 2.2965
Learning rate: 0.000923
Epoch [17], Loss: 2.2956
Learning rate: 0.000918
Epoch [18], Loss: 2.2946
Learning rate: 0.000914
Epoch [19], Loss: 2.2934
Learning rate: 0.000909
Epoch [20], Loss: 2.2917
Learning rate: 0.000905
Epoch [21], Loss: 2.2891
Learning rate: 0.000900
Epoch [22], Loss: 2.2869
Learning rate: 0.000896
Epoch [23], Loss: 2.2840
Learning rate: 0.000891
Epoch [24], Loss: 2.2804
Learning rate: 0.000887
Epoch [25], Loss: 2.2768
Learning rate: 0.000882
Epoch [26], Loss: 2.2724
Learning rate: 0.000878
Epoch [27], Loss: 2.2691
Learning rate: 0.000873
Epoch [28], Loss: 2.2636
Learning rate: 0.000869
Epoch [29], Loss: 2.2573
Learning rate: 0.000865
Epoch [30], Loss: 2.2512
Learning rate: 0.000860
Epoch [31], Loss: 2.2448
Learning rate: 0.000856
Epoch [32], Loss: 2.2372
Learning rate: 0.000852
Epoch [33], Loss: 2.2283
Learning rate: 0.000848
Epoch [34], Loss: 2.2207
Learning rate: 0.000843
Epoch [35], Loss: 2.2101
Learning rate: 0.000839
Epoch [36], Loss: 2.1999
Learning rate: 0.000835
Epoch [37], Loss: 2.1879
Learning rate: 0.000831
Epoch [38], Loss: 2.1752
Learning rate: 0.000827
Epoch [39], Loss: 2.1608
Learning rate: 0.000822
Epoch [40], Loss: 2.1475
Learning rate: 0.000818
Epoch [41], Loss: 2.1330
Learning rate: 0.000814
Epoch [42], Loss: 2.1184
Learning rate: 0.000810
Epoch [43], Loss: 2.1034
Learning rate: 0.000806
Epoch [44], Loss: 2.0857
Learning rate: 0.000802
Epoch [45], Loss: 2.0691
Learning rate: 0.000798
Epoch [46], Loss: 2.0536
Learning rate: 0.000794
Epoch [47], Loss: 2.0351
Learning rate: 0.000790
Epoch [48], Loss: 2.0159
Learning rate: 0.000786
Epoch [49], Loss: 1.9959
Learning rate: 0.000782
Epoch [50], Loss: 1.9779
Learning rate: 0.000778
Epoch [51], Loss: 1.9592
Learning rate: 0.000774
Epoch [52], Loss: 1.9370
Learning rate: 0.000771
Epoch [53], Loss: 1.9176
Learning rate: 0.000767
Epoch [54], Loss: 1.8936
Learning rate: 0.000763
Epoch [55], Loss: 1.8757
Learning rate: 0.000759
Epoch [56], Loss: 1.8538
Learning rate: 0.000755
Epoch [57], Loss: 1.8324
Learning rate: 0.000751
Epoch [58], Loss: 1.8129
Learning rate: 0.000748
Epoch [59], Loss: 1.7928
Learning rate: 0.000744
Epoch [60], Loss: 1.7690
Learning rate: 0.000740
Epoch [61], Loss: 1.7491
Learning rate: 0.000737
Epoch [62], Loss: 1.7299
Learning rate: 0.000733
Epoch [63], Loss: 1.7073
Learning rate: 0.000729
Epoch [64], Loss: 1.6866
Learning rate: 0.000726
Epoch [65], Loss: 1.6681
Learning rate: 0.000722
Epoch [66], Loss: 1.6484
Learning rate: 0.000718
Epoch [67], Loss: 1.6267
Learning rate: 0.000715
Epoch [68], Loss: 1.6081
Learning rate: 0.000711
Epoch [69], Loss: 1.5911
Learning rate: 0.000708
Epoch [70], Loss: 1.5721
Learning rate: 0.000704
Epoch [71], Loss: 1.5538
Learning rate: 0.000701
Epoch [72], Loss: 1.5342
Learning rate: 0.000697
Epoch [73], Loss: 1.5146
Learning rate: 0.000694
Epoch [74], Loss: 1.5010
Learning rate: 0.000690
Epoch [75], Loss: 1.4796
Learning rate: 0.000687
Epoch [76], Loss: 1.4638
Learning rate: 0.000683
Epoch [77], Loss: 1.4459
Learning rate: 0.000680
Epoch [78], Loss: 1.4288
Learning rate: 0.000676
Epoch [79], Loss: 1.4132
Learning rate: 0.000673
Epoch [80], Loss: 1.3959
Learning rate: 0.000670
Epoch [81], Loss: 1.3852
Learning rate: 0.000666
Epoch [82], Loss: 1.3701
Learning rate: 0.000663
Epoch [83], Loss: 1.3513
Learning rate: 0.000660
Epoch [84], Loss: 1.3346
Learning rate: 0.000656
Epoch [85], Loss: 1.3209
Learning rate: 0.000653
Epoch [86], Loss: 1.3068
Learning rate: 0.000650
Epoch [87], Loss: 1.2912
Learning rate: 0.000647
Epoch [88], Loss: 1.2814
Learning rate: 0.000643
Epoch [89], Loss: 1.2639
Learning rate: 0.000640
Epoch [90], Loss: 1.2501
Learning rate: 0.000637
Epoch [91], Loss: 1.2371
Learning rate: 0.000634
Epoch [92], Loss: 1.2222
Learning rate: 0.000631
Epoch [93], Loss: 1.2125
Learning rate: 0.000627
Epoch [94], Loss: 1.1996
Learning rate: 0.000624
Epoch [95], Loss: 1.1878
Learning rate: 0.000621
Epoch [96], Loss: 1.1754
Learning rate: 0.000618
Epoch [97], Loss: 1.1625
Learning rate: 0.000615
Epoch [98], Loss: 1.1512
Learning rate: 0.000612
Epoch [99], Loss: 1.1391
Learning rate: 0.000609
Epoch [100], Loss: 1.1273
Learning rate: 0.000606
Epoch [101], Loss: 1.1176
Learning rate: 0.000603
Epoch [102], Loss: 1.1070
Learning rate: 0.000600
Randomizing 100% of the dataset
Train set size 60000
Validation set size 0
Test set size 10000
Epoch [1], Loss: 2.4335
Learning rate: 0.000995
Epoch [2], Loss: 2.3021
Learning rate: 0.000990
Epoch [3], Loss: 2.3019
Learning rate: 0.000985
Epoch [4], Loss: 2.3014
Learning rate: 0.000980
Epoch [5], Loss: 2.3013
Learning rate: 0.000975
Epoch [6], Loss: 2.3012
Learning rate: 0.000970
Epoch [7], Loss: 2.3009
Learning rate: 0.000966
Epoch [8], Loss: 2.3008
Learning rate: 0.000961
Epoch [9], Loss: 2.3005
Learning rate: 0.000956
Epoch [10], Loss: 2.3001
Learning rate: 0.000951
Epoch [11], Loss: 2.3000
Learning rate: 0.000946
Epoch [12], Loss: 2.2996
Learning rate: 0.000942
Epoch [13], Loss: 2.2993
Learning rate: 0.000937
Epoch [14], Loss: 2.2992
Learning rate: 0.000932
Epoch [15], Loss: 2.2989
Learning rate: 0.000928
Epoch [16], Loss: 2.2983
Learning rate: 0.000923
Epoch [17], Loss: 2.2982
Learning rate: 0.000918
Epoch [18], Loss: 2.2977
Learning rate: 0.000914
Epoch [19], Loss: 2.2970
Learning rate: 0.000909
Epoch [20], Loss: 2.2966
Learning rate: 0.000905
Epoch [21], Loss: 2.2957
Learning rate: 0.000900
Epoch [22], Loss: 2.2953
Learning rate: 0.000896
Epoch [23], Loss: 2.2951
Learning rate: 0.000891
Epoch [24], Loss: 2.2949
Learning rate: 0.000887
Epoch [25], Loss: 2.2939
Learning rate: 0.000882
Epoch [26], Loss: 2.2936
Learning rate: 0.000878
Epoch [27], Loss: 2.2927
Learning rate: 0.000873
Epoch [28], Loss: 2.2918
Learning rate: 0.000869
Epoch [29], Loss: 2.2912
Learning rate: 0.000865
Epoch [30], Loss: 2.2905
Learning rate: 0.000860
Epoch [31], Loss: 2.2897
Learning rate: 0.000856
Epoch [32], Loss: 2.2893
Learning rate: 0.000852
Epoch [33], Loss: 2.2886
Learning rate: 0.000848
Epoch [34], Loss: 2.2877
Learning rate: 0.000843
Epoch [35], Loss: 2.2866
Learning rate: 0.000839
Epoch [36], Loss: 2.2854
Learning rate: 0.000835
Epoch [37], Loss: 2.2844
Learning rate: 0.000831
Epoch [38], Loss: 2.2832
Learning rate: 0.000827
Epoch [39], Loss: 2.2823
Learning rate: 0.000822
Epoch [40], Loss: 2.2813
Learning rate: 0.000818
Epoch [41], Loss: 2.2795
Learning rate: 0.000814
Epoch [42], Loss: 2.2780
Learning rate: 0.000810
Epoch [43], Loss: 2.2773
Learning rate: 0.000806
Epoch [44], Loss: 2.2754
Learning rate: 0.000802
Epoch [45], Loss: 2.2736
Learning rate: 0.000798
Epoch [46], Loss: 2.2714
Learning rate: 0.000794
Epoch [47], Loss: 2.2701
Learning rate: 0.000790
Epoch [48], Loss: 2.2680
Learning rate: 0.000786
Epoch [49], Loss: 2.2663
Learning rate: 0.000782
Epoch [50], Loss: 2.2645
Learning rate: 0.000778
Epoch [51], Loss: 2.2617
Learning rate: 0.000774
Epoch [52], Loss: 2.2610
Learning rate: 0.000771
Epoch [53], Loss: 2.2590
Learning rate: 0.000767
Epoch [54], Loss: 2.2562
Learning rate: 0.000763
Epoch [55], Loss: 2.2559
Learning rate: 0.000759
Epoch [56], Loss: 2.2531
Learning rate: 0.000755
Epoch [57], Loss: 2.2500
Learning rate: 0.000751
Epoch [58], Loss: 2.2471
Learning rate: 0.000748
Epoch [59], Loss: 2.2443
Learning rate: 0.000744
Epoch [60], Loss: 2.2419
Learning rate: 0.000740
Epoch [61], Loss: 2.2396
Learning rate: 0.000737
Epoch [62], Loss: 2.2365
Learning rate: 0.000733
Epoch [63], Loss: 2.2332
Learning rate: 0.000729
Epoch [64], Loss: 2.2308
Learning rate: 0.000726
Epoch [65], Loss: 2.2289
Learning rate: 0.000722
Epoch [66], Loss: 2.2256
Learning rate: 0.000718
Epoch [67], Loss: 2.2232
Learning rate: 0.000715
Epoch [68], Loss: 2.2207
Learning rate: 0.000711
Epoch [69], Loss: 2.2171
Learning rate: 0.000708
Epoch [70], Loss: 2.2138
Learning rate: 0.000704
Epoch [71], Loss: 2.2110
Learning rate: 0.000701
Epoch [72], Loss: 2.2094
Learning rate: 0.000697
Epoch [73], Loss: 2.2063
Learning rate: 0.000694
Epoch [74], Loss: 2.2013
Learning rate: 0.000690
Epoch [75], Loss: 2.1989
Learning rate: 0.000687
Epoch [76], Loss: 2.1982
Learning rate: 0.000683
Epoch [77], Loss: 2.1934
Learning rate: 0.000680
Epoch [78], Loss: 2.1901
Learning rate: 0.000676
Epoch [79], Loss: 2.1862
Learning rate: 0.000673
Epoch [80], Loss: 2.1826
Learning rate: 0.000670
Epoch [81], Loss: 2.1788
Learning rate: 0.000666
Epoch [82], Loss: 2.1749
Learning rate: 0.000663
Epoch [83], Loss: 2.1724
Learning rate: 0.000660
Epoch [84], Loss: 2.1681
Learning rate: 0.000656
Epoch [85], Loss: 2.1663
Learning rate: 0.000653
Epoch [86], Loss: 2.1621
Learning rate: 0.000650
Epoch [87], Loss: 2.1577
Learning rate: 0.000647
Epoch [88], Loss: 2.1528
Learning rate: 0.000643
Epoch [89], Loss: 2.1494
Learning rate: 0.000640
Epoch [90], Loss: 2.1458
Learning rate: 0.000637
Epoch [91], Loss: 2.1417
Learning rate: 0.000634
Epoch [92], Loss: 2.1385
Learning rate: 0.000631
Epoch [93], Loss: 2.1342
Learning rate: 0.000627
Epoch [94], Loss: 2.1298
Learning rate: 0.000624
Epoch [95], Loss: 2.1261
Learning rate: 0.000621
Epoch [96], Loss: 2.1224
Learning rate: 0.000618
Epoch [97], Loss: 2.1198
Learning rate: 0.000615
Epoch [98], Loss: 2.1133
Learning rate: 0.000612
Epoch [99], Loss: 2.1109
Learning rate: 0.000609
Epoch [100], Loss: 2.1059
Learning rate: 0.000606
Epoch [101], Loss: 2.1037
Learning rate: 0.000603
Epoch [102], Loss: 2.0985
Learning rate: 0.000600
Randomizing 100% of the dataset
Train set size 60000
Validation set size 0
Test set size 10000
Epoch [1], Loss: 2.3403
Learning rate: 0.000995
Epoch [2], Loss: 2.3019
Learning rate: 0.000990
Epoch [3], Loss: 2.3018
Learning rate: 0.000985
Epoch [4], Loss: 2.3017
Learning rate: 0.000980
Epoch [5], Loss: 2.3016
Learning rate: 0.000975
Epoch [6], Loss: 2.3014
Learning rate: 0.000970
Epoch [7], Loss: 2.3016
Learning rate: 0.000966
Epoch [8], Loss: 2.3013
Learning rate: 0.000961
Epoch [9], Loss: 2.3011
Learning rate: 0.000956
Epoch [10], Loss: 2.3011
Learning rate: 0.000951
Epoch [11], Loss: 2.3010
Learning rate: 0.000946
Epoch [12], Loss: 2.3013
Learning rate: 0.000942
Epoch [13], Loss: 2.3010
Learning rate: 0.000937
Epoch [14], Loss: 2.3009
Learning rate: 0.000932
Epoch [15], Loss: 2.3006
Learning rate: 0.000928
Epoch [16], Loss: 2.3006
Learning rate: 0.000923
Epoch [17], Loss: 2.3004
Learning rate: 0.000918
Epoch [18], Loss: 2.3003
Learning rate: 0.000914
Epoch [19], Loss: 2.3003
Learning rate: 0.000909
Epoch [20], Loss: 2.3001
Learning rate: 0.000905
Epoch [21], Loss: 2.3000
Learning rate: 0.000900
Epoch [22], Loss: 2.2997
Learning rate: 0.000896
Epoch [23], Loss: 2.2996
Learning rate: 0.000891
Epoch [24], Loss: 2.2994
Learning rate: 0.000887
Epoch [25], Loss: 2.2990
Learning rate: 0.000882
Epoch [26], Loss: 2.2991
Learning rate: 0.000878
Epoch [27], Loss: 2.2988
Learning rate: 0.000873
Epoch [28], Loss: 2.2985
Learning rate: 0.000869
Epoch [29], Loss: 2.2986
Learning rate: 0.000865
Epoch [30], Loss: 2.2981
Learning rate: 0.000860
Epoch [31], Loss: 2.2981
Learning rate: 0.000856
Epoch [32], Loss: 2.2972
Learning rate: 0.000852
Epoch [33], Loss: 2.2972
Learning rate: 0.000848
Epoch [34], Loss: 2.2973
Learning rate: 0.000843
Epoch [35], Loss: 2.2970
Learning rate: 0.000839
Epoch [36], Loss: 2.2968
Learning rate: 0.000835
Epoch [37], Loss: 2.2966
Learning rate: 0.000831
Epoch [38], Loss: 2.2957
Learning rate: 0.000827
Epoch [39], Loss: 2.2956
Learning rate: 0.000822
Epoch [40], Loss: 2.2950
Learning rate: 0.000818
Epoch [41], Loss: 2.2943
Learning rate: 0.000814
Epoch [42], Loss: 2.2942
Learning rate: 0.000810
Epoch [43], Loss: 2.2942
Learning rate: 0.000806
Epoch [44], Loss: 2.2936
Learning rate: 0.000802
Epoch [45], Loss: 2.2933
Learning rate: 0.000798
Epoch [46], Loss: 2.2930
Learning rate: 0.000794
Epoch [47], Loss: 2.2926
Learning rate: 0.000790
Epoch [48], Loss: 2.2920
Learning rate: 0.000786
Epoch [49], Loss: 2.2926
Learning rate: 0.000782
Epoch [50], Loss: 2.2916
Learning rate: 0.000778
Epoch [51], Loss: 2.2910
Learning rate: 0.000774
Epoch [52], Loss: 2.2907
Learning rate: 0.000771
Epoch [53], Loss: 2.2904
Learning rate: 0.000767
Epoch [54], Loss: 2.2898
Learning rate: 0.000763
Epoch [55], Loss: 2.2892
Learning rate: 0.000759
Epoch [56], Loss: 2.2887
Learning rate: 0.000755
Epoch [57], Loss: 2.2884
Learning rate: 0.000751
Epoch [58], Loss: 2.2889
Learning rate: 0.000748
Epoch [59], Loss: 2.2876
Learning rate: 0.000744
Epoch [60], Loss: 2.2866
Learning rate: 0.000740
Epoch [61], Loss: 2.2869
Learning rate: 0.000737
Epoch [62], Loss: 2.2860
Learning rate: 0.000733
Epoch [63], Loss: 2.2853
Learning rate: 0.000729
Epoch [64], Loss: 2.2848
Learning rate: 0.000726
Epoch [65], Loss: 2.2839
Learning rate: 0.000722
Epoch [66], Loss: 2.2836
Learning rate: 0.000718
Epoch [67], Loss: 2.2832
Learning rate: 0.000715
Epoch [68], Loss: 2.2830
Learning rate: 0.000711
Epoch [69], Loss: 2.2819
Learning rate: 0.000708
Epoch [70], Loss: 2.2815
Learning rate: 0.000704
Epoch [71], Loss: 2.2818
Learning rate: 0.000701
Epoch [72], Loss: 2.2809
Learning rate: 0.000697
Epoch [73], Loss: 2.2802
Learning rate: 0.000694
Epoch [74], Loss: 2.2791
Learning rate: 0.000690
Epoch [75], Loss: 2.2796
Learning rate: 0.000687
Epoch [76], Loss: 2.2788
Learning rate: 0.000683
Epoch [77], Loss: 2.2786
Learning rate: 0.000680
Epoch [78], Loss: 2.2781
Learning rate: 0.000676
Epoch [79], Loss: 2.2773
Learning rate: 0.000673
Epoch [80], Loss: 2.2773
Learning rate: 0.000670
Epoch [81], Loss: 2.2766
Learning rate: 0.000666
Epoch [82], Loss: 2.2760
Learning rate: 0.000663
Epoch [83], Loss: 2.2760
Learning rate: 0.000660
Epoch [84], Loss: 2.2754
Learning rate: 0.000656
Epoch [85], Loss: 2.2750
Learning rate: 0.000653
Epoch [86], Loss: 2.2745
Learning rate: 0.000650
Epoch [87], Loss: 2.2738
Learning rate: 0.000647
Epoch [88], Loss: 2.2739
Learning rate: 0.000643
Epoch [89], Loss: 2.2738
Learning rate: 0.000640
Epoch [90], Loss: 2.2731
Learning rate: 0.000637
Epoch [91], Loss: 2.2727
Learning rate: 0.000634
Epoch [92], Loss: 2.2720
Learning rate: 0.000631
Epoch [93], Loss: 2.2719
Learning rate: 0.000627
Epoch [94], Loss: 2.2717
Learning rate: 0.000624
Epoch [95], Loss: 2.2716
Learning rate: 0.000621
Epoch [96], Loss: 2.2713
Learning rate: 0.000618
Epoch [97], Loss: 2.2706
Learning rate: 0.000615
Epoch [98], Loss: 2.2698
Learning rate: 0.000612
Epoch [99], Loss: 2.2699
Learning rate: 0.000609
Epoch [100], Loss: 2.2698
Learning rate: 0.000606
Epoch [101], Loss: 2.2693
Learning rate: 0.000603
Epoch [102], Loss: 2.2689
Learning rate: 0.000600
