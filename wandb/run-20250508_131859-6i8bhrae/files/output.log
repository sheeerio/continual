Randomizing 100% of the dataset
Train set size 60000
Validation set size 0
Test set size 10000
Epoch [1], Loss: 2.3036
Learning rate: 0.000995
Epoch [2], Loss: 2.3020
Learning rate: 0.000990
Epoch [3], Loss: 2.3002
Learning rate: 0.000985
Epoch [4], Loss: 2.2967
Learning rate: 0.000980
Epoch [5], Loss: 2.2904
Learning rate: 0.000975
Epoch [6], Loss: 2.2803
Learning rate: 0.000970
Epoch [7], Loss: 2.2655
Learning rate: 0.000966
Epoch [8], Loss: 2.2455
Learning rate: 0.000961
Epoch [9], Loss: 2.2195
Learning rate: 0.000956
Epoch [10], Loss: 2.1879
Learning rate: 0.000951
Epoch [11], Loss: 2.1505
Learning rate: 0.000946
Epoch [12], Loss: 2.1100
Learning rate: 0.000942
Epoch [13], Loss: 2.0650
Learning rate: 0.000937
Epoch [14], Loss: 2.0170
Learning rate: 0.000932
Epoch [15], Loss: 1.9711
Learning rate: 0.000928
Epoch [16], Loss: 1.9206
Learning rate: 0.000923
Epoch [17], Loss: 1.8715
Learning rate: 0.000918
Epoch [18], Loss: 1.8194
Learning rate: 0.000914
Epoch [19], Loss: 1.7677
Learning rate: 0.000909
Epoch [20], Loss: 1.7184
Learning rate: 0.000905
Epoch [21], Loss: 1.6663
Learning rate: 0.000900
Epoch [22], Loss: 1.6190
Learning rate: 0.000896
Epoch [23], Loss: 1.5689
Learning rate: 0.000891
Epoch [24], Loss: 1.5207
Learning rate: 0.000887
Epoch [25], Loss: 1.4710
Learning rate: 0.000882
Epoch [26], Loss: 1.4193
Learning rate: 0.000878
Epoch [27], Loss: 1.3784
Learning rate: 0.000873
Epoch [28], Loss: 1.3353
Learning rate: 0.000869
Epoch [29], Loss: 1.2910
Learning rate: 0.000865
Epoch [30], Loss: 1.2492
Learning rate: 0.000860
Epoch [31], Loss: 1.2097
Learning rate: 0.000856
Epoch [32], Loss: 1.1637
Learning rate: 0.000852
Epoch [33], Loss: 1.1250
Learning rate: 0.000848
Epoch [34], Loss: 1.0866
Learning rate: 0.000843
Epoch [35], Loss: 1.0497
Learning rate: 0.000839
Epoch [36], Loss: 1.0133
Learning rate: 0.000835
Epoch [37], Loss: 0.9745
Learning rate: 0.000831
Epoch [38], Loss: 0.9436
Learning rate: 0.000827
Epoch [39], Loss: 0.9083
Learning rate: 0.000822
Epoch [40], Loss: 0.8787
Learning rate: 0.000818
Epoch [41], Loss: 0.8446
Learning rate: 0.000814
Epoch [42], Loss: 0.8110
Learning rate: 0.000810
Epoch [43], Loss: 0.7862
Learning rate: 0.000806
Epoch [44], Loss: 0.7537
Learning rate: 0.000802
Epoch [45], Loss: 0.7255
Learning rate: 0.000798
Epoch [46], Loss: 0.6978
Learning rate: 0.000794
Epoch [47], Loss: 0.6747
Learning rate: 0.000790
Epoch [48], Loss: 0.6496
Learning rate: 0.000786
Epoch [49], Loss: 0.6247
Learning rate: 0.000782
Epoch [50], Loss: 0.5998
Learning rate: 0.000778
Epoch [51], Loss: 0.5761
Learning rate: 0.000774
Epoch [52], Loss: 0.5566
Learning rate: 0.000771
Epoch [53], Loss: 0.5337
Learning rate: 0.000767
Epoch [54], Loss: 0.5119
Learning rate: 0.000763
Epoch [55], Loss: 0.4927
Learning rate: 0.000759
Epoch [56], Loss: 0.4741
Learning rate: 0.000755
Epoch [57], Loss: 0.4566
Learning rate: 0.000751
Epoch [58], Loss: 0.4388
Learning rate: 0.000748
Epoch [59], Loss: 0.4208
Learning rate: 0.000744
Epoch [60], Loss: 0.4048
Learning rate: 0.000740
Epoch [61], Loss: 0.3891
Learning rate: 0.000737
Epoch [62], Loss: 0.3734
Learning rate: 0.000733
Epoch [63], Loss: 0.3626
Learning rate: 0.000729
Epoch [64], Loss: 0.3448
Learning rate: 0.000726
Epoch [65], Loss: 0.3320
Learning rate: 0.000722
Epoch [66], Loss: 0.3219
Learning rate: 0.000718
Epoch [67], Loss: 0.3100
Learning rate: 0.000715
Epoch [68], Loss: 0.2975
Learning rate: 0.000711
Epoch [69], Loss: 0.2875
Learning rate: 0.000708
Epoch [70], Loss: 0.2740
Learning rate: 0.000704
Epoch [71], Loss: 0.2677
Learning rate: 0.000701
Epoch [72], Loss: 0.2551
Learning rate: 0.000697
Epoch [73], Loss: 0.2479
Learning rate: 0.000694
Epoch [74], Loss: 0.2387
Learning rate: 0.000690
Epoch [75], Loss: 0.2311
Learning rate: 0.000687
Epoch [76], Loss: 0.2205
Learning rate: 0.000683
Epoch [77], Loss: 0.2153
Learning rate: 0.000680
Epoch [78], Loss: 0.2088
Learning rate: 0.000676
Epoch [79], Loss: 0.2016
Learning rate: 0.000673
Epoch [80], Loss: 0.1945
Learning rate: 0.000670
Epoch [81], Loss: 0.1868
Learning rate: 0.000666
Epoch [82], Loss: 0.1814
Learning rate: 0.000663
Epoch [83], Loss: 0.1743
Learning rate: 0.000660
Epoch [84], Loss: 0.1693
Learning rate: 0.000656
Epoch [85], Loss: 0.1644
Learning rate: 0.000653
Epoch [86], Loss: 0.1578
Learning rate: 0.000650
Epoch [87], Loss: 0.1536
Learning rate: 0.000647
Epoch [88], Loss: 0.1467
Learning rate: 0.000643
Epoch [89], Loss: 0.1448
Learning rate: 0.000640
Epoch [90], Loss: 0.1392
Learning rate: 0.000637
Epoch [91], Loss: 0.1377
Learning rate: 0.000634
Epoch [92], Loss: 0.1307
Learning rate: 0.000631
Epoch [93], Loss: 0.1276
Learning rate: 0.000627
Epoch [94], Loss: 0.1220
Learning rate: 0.000624
Epoch [95], Loss: 0.1173
Learning rate: 0.000621
Epoch [96], Loss: 0.1175
Learning rate: 0.000618
Epoch [97], Loss: 0.1124
Learning rate: 0.000615
Epoch [98], Loss: 0.1066
Learning rate: 0.000612
Epoch [99], Loss: 0.1060
Learning rate: 0.000609
Epoch [100], Loss: 0.1029
Learning rate: 0.000606
Epoch [101], Loss: 0.1007
Learning rate: 0.000603
Epoch [102], Loss: 0.0963
Learning rate: 0.000600
Randomizing 100% of the dataset
Train set size 60000
Validation set size 0
Test set size 10000
Epoch [1], Loss: 3.6565
Learning rate: 0.000995
Epoch [2], Loss: 2.3036
Learning rate: 0.000990
Epoch [3], Loss: 2.2964
Learning rate: 0.000985
Epoch [4], Loss: 2.2869
Learning rate: 0.000980
Epoch [5], Loss: 2.2736
Learning rate: 0.000975
Epoch [6], Loss: 2.2595
Learning rate: 0.000970
Epoch [7], Loss: 2.2417
Learning rate: 0.000966
Epoch [8], Loss: 2.2225
Learning rate: 0.000961
Epoch [9], Loss: 2.1998
Learning rate: 0.000956
Epoch [10], Loss: 2.1759
Learning rate: 0.000951
Epoch [11], Loss: 2.1519
Learning rate: 0.000946
Epoch [12], Loss: 2.1251
Learning rate: 0.000942
Epoch [13], Loss: 2.0975
Learning rate: 0.000937
Epoch [14], Loss: 2.0695
Learning rate: 0.000932
Epoch [15], Loss: 2.0417
Learning rate: 0.000928
Epoch [16], Loss: 2.0116
Learning rate: 0.000923
Epoch [17], Loss: 1.9838
Learning rate: 0.000918
Epoch [18], Loss: 1.9531
Learning rate: 0.000914
Epoch [19], Loss: 1.9250
Learning rate: 0.000909
Epoch [20], Loss: 1.8952
Learning rate: 0.000905
Epoch [21], Loss: 1.8671
Learning rate: 0.000900
Epoch [22], Loss: 1.8378
Learning rate: 0.000896
Epoch [23], Loss: 1.8108
Learning rate: 0.000891
Epoch [24], Loss: 1.7826
Learning rate: 0.000887
Epoch [25], Loss: 1.7553
Learning rate: 0.000882
Epoch [26], Loss: 1.7241
Learning rate: 0.000878
Epoch [27], Loss: 1.7007
Learning rate: 0.000873
Epoch [28], Loss: 1.6755
Learning rate: 0.000869
Epoch [29], Loss: 1.6485
Learning rate: 0.000865
Epoch [30], Loss: 1.6231
Learning rate: 0.000860
Epoch [31], Loss: 1.5971
Learning rate: 0.000856
Epoch [32], Loss: 1.5725
Learning rate: 0.000852
Epoch [33], Loss: 1.5515
Learning rate: 0.000848
Epoch [34], Loss: 1.5271
Learning rate: 0.000843
Epoch [35], Loss: 1.5043
Learning rate: 0.000839
Epoch [36], Loss: 1.4816
Learning rate: 0.000835
Epoch [37], Loss: 1.4605
Learning rate: 0.000831
Epoch [38], Loss: 1.4386
Learning rate: 0.000827
Epoch [39], Loss: 1.4168
Learning rate: 0.000822
Epoch [40], Loss: 1.3957
Learning rate: 0.000818
Epoch [41], Loss: 1.3777
Learning rate: 0.000814
Epoch [42], Loss: 1.3586
Learning rate: 0.000810
Epoch [43], Loss: 1.3385
Learning rate: 0.000806
Epoch [44], Loss: 1.3193
Learning rate: 0.000802
Epoch [45], Loss: 1.3015
Learning rate: 0.000798
Epoch [46], Loss: 1.2830
Learning rate: 0.000794
Epoch [47], Loss: 1.2650
Learning rate: 0.000790
Epoch [48], Loss: 1.2460
Learning rate: 0.000786
Epoch [49], Loss: 1.2322
Learning rate: 0.000782
Epoch [50], Loss: 1.2160
Learning rate: 0.000778
Epoch [51], Loss: 1.1999
Learning rate: 0.000774
Epoch [52], Loss: 1.1844
Learning rate: 0.000771
Epoch [53], Loss: 1.1694
Learning rate: 0.000767
Epoch [54], Loss: 1.1535
Learning rate: 0.000763
Epoch [55], Loss: 1.1397
Learning rate: 0.000759
Epoch [56], Loss: 1.1246
Learning rate: 0.000755
Epoch [57], Loss: 1.1106
Learning rate: 0.000751
Epoch [58], Loss: 1.0987
Learning rate: 0.000748
Epoch [59], Loss: 1.0847
Learning rate: 0.000744
Epoch [60], Loss: 1.0721
Learning rate: 0.000740
Epoch [61], Loss: 1.0597
Learning rate: 0.000737
Epoch [62], Loss: 1.0464
Learning rate: 0.000733
Epoch [63], Loss: 1.0352
Learning rate: 0.000729
Epoch [64], Loss: 1.0244
Learning rate: 0.000726
Epoch [65], Loss: 1.0118
Learning rate: 0.000722
Epoch [66], Loss: 1.0016
Learning rate: 0.000718
Epoch [67], Loss: 0.9884
Learning rate: 0.000715
Epoch [68], Loss: 0.9785
Learning rate: 0.000711
Epoch [69], Loss: 0.9686
Learning rate: 0.000708
Epoch [70], Loss: 0.9577
Learning rate: 0.000704
Epoch [71], Loss: 0.9476
Learning rate: 0.000701
Epoch [72], Loss: 0.9375
Learning rate: 0.000697
Epoch [73], Loss: 0.9282
Learning rate: 0.000694
Epoch [74], Loss: 0.9188
Learning rate: 0.000690
Epoch [75], Loss: 0.9083
Learning rate: 0.000687
Epoch [76], Loss: 0.8992
Learning rate: 0.000683
Epoch [77], Loss: 0.8898
Learning rate: 0.000680
Epoch [78], Loss: 0.8801
Learning rate: 0.000676
Epoch [79], Loss: 0.8712
Learning rate: 0.000673
Epoch [80], Loss: 0.8626
Learning rate: 0.000670
Epoch [81], Loss: 0.8550
Learning rate: 0.000666
Epoch [82], Loss: 0.8470
Learning rate: 0.000663
Epoch [83], Loss: 0.8388
Learning rate: 0.000660
Epoch [84], Loss: 0.8308
Learning rate: 0.000656
Epoch [85], Loss: 0.8224
Learning rate: 0.000653
Epoch [86], Loss: 0.8181
Learning rate: 0.000650
Epoch [87], Loss: 0.8088
Learning rate: 0.000647
Epoch [88], Loss: 0.8015
Learning rate: 0.000643
Epoch [89], Loss: 0.7933
Learning rate: 0.000640
Epoch [90], Loss: 0.7875
Learning rate: 0.000637
Epoch [91], Loss: 0.7784
Learning rate: 0.000634
Epoch [92], Loss: 0.7731
Learning rate: 0.000631
Epoch [93], Loss: 0.7670
Learning rate: 0.000627
Epoch [94], Loss: 0.7594
Learning rate: 0.000624
Epoch [95], Loss: 0.7520
Learning rate: 0.000621
Epoch [96], Loss: 0.7466
Learning rate: 0.000618
Epoch [97], Loss: 0.7386
Learning rate: 0.000615
Epoch [98], Loss: 0.7355
Learning rate: 0.000612
Epoch [99], Loss: 0.7282
Learning rate: 0.000609
Epoch [100], Loss: 0.7223
Learning rate: 0.000606
Epoch [101], Loss: 0.7178
Learning rate: 0.000603
Epoch [102], Loss: 0.7124
Learning rate: 0.000600
Randomizing 100% of the dataset
Train set size 60000
Validation set size 0
Test set size 10000
Epoch [1], Loss: 2.8718
Learning rate: 0.000995
Epoch [2], Loss: 2.3036
Learning rate: 0.000990
Epoch [3], Loss: 2.2991
Learning rate: 0.000985
Epoch [4], Loss: 2.2959
Learning rate: 0.000980
Epoch [5], Loss: 2.2941
Learning rate: 0.000975
Epoch [6], Loss: 2.2911
Learning rate: 0.000970
Epoch [7], Loss: 2.2878
Learning rate: 0.000966
Epoch [8], Loss: 2.2844
Learning rate: 0.000961
Epoch [9], Loss: 2.2812
Learning rate: 0.000956
Epoch [10], Loss: 2.2779
Learning rate: 0.000951
Epoch [11], Loss: 2.2735
Learning rate: 0.000946
Epoch [12], Loss: 2.2685
Learning rate: 0.000942
Epoch [13], Loss: 2.2644
Learning rate: 0.000937
Epoch [14], Loss: 2.2585
Learning rate: 0.000932
Epoch [15], Loss: 2.2542
Learning rate: 0.000928
Epoch [16], Loss: 2.2477
Learning rate: 0.000923
Epoch [17], Loss: 2.2421
Learning rate: 0.000918
Epoch [18], Loss: 2.2357
Learning rate: 0.000914
Epoch [19], Loss: 2.2295
Learning rate: 0.000909
Epoch [20], Loss: 2.2223
Learning rate: 0.000905
Epoch [21], Loss: 2.2158
Learning rate: 0.000900
Epoch [22], Loss: 2.2088
Learning rate: 0.000896
Epoch [23], Loss: 2.2023
Learning rate: 0.000891
Epoch [24], Loss: 2.1950
Learning rate: 0.000887
Epoch [25], Loss: 2.1885
Learning rate: 0.000882
Epoch [26], Loss: 2.1790
Learning rate: 0.000878
Epoch [27], Loss: 2.1727
Learning rate: 0.000873
Epoch [28], Loss: 2.1657
Learning rate: 0.000869
Epoch [29], Loss: 2.1569
Learning rate: 0.000865
Epoch [30], Loss: 2.1496
Learning rate: 0.000860
Epoch [31], Loss: 2.1430
Learning rate: 0.000856
Epoch [32], Loss: 2.1355
Learning rate: 0.000852
Epoch [33], Loss: 2.1272
Learning rate: 0.000848
Epoch [34], Loss: 2.1200
Learning rate: 0.000843
Epoch [35], Loss: 2.1127
Learning rate: 0.000839
Epoch [36], Loss: 2.1041
Learning rate: 0.000835
Epoch [37], Loss: 2.0977
Learning rate: 0.000831
Epoch [38], Loss: 2.0901
Learning rate: 0.000827
Epoch [39], Loss: 2.0838
Learning rate: 0.000822
Epoch [40], Loss: 2.0754
Learning rate: 0.000818
Epoch [41], Loss: 2.0686
Learning rate: 0.000814
Epoch [42], Loss: 2.0618
Learning rate: 0.000810
Epoch [43], Loss: 2.0547
Learning rate: 0.000806
Epoch [44], Loss: 2.0477
Learning rate: 0.000802
Epoch [45], Loss: 2.0407
Learning rate: 0.000798
Epoch [46], Loss: 2.0327
Learning rate: 0.000794
Epoch [47], Loss: 2.0271
Learning rate: 0.000790
Epoch [48], Loss: 2.0195
Learning rate: 0.000786
Epoch [49], Loss: 2.0128
Learning rate: 0.000782
Epoch [50], Loss: 2.0043
Learning rate: 0.000778
Epoch [51], Loss: 1.9995
Learning rate: 0.000774
Epoch [52], Loss: 1.9935
Learning rate: 0.000771
Epoch [53], Loss: 1.9867
Learning rate: 0.000767
Epoch [54], Loss: 1.9808
Learning rate: 0.000763
Epoch [55], Loss: 1.9730
Learning rate: 0.000759
Epoch [56], Loss: 1.9666
Learning rate: 0.000755
Epoch [57], Loss: 1.9604
Learning rate: 0.000751
Epoch [58], Loss: 1.9538
Learning rate: 0.000748
Epoch [59], Loss: 1.9465
Learning rate: 0.000744
Epoch [60], Loss: 1.9412
Learning rate: 0.000740
Epoch [61], Loss: 1.9346
Learning rate: 0.000737
Epoch [62], Loss: 1.9286
Learning rate: 0.000733
Epoch [63], Loss: 1.9227
Learning rate: 0.000729
Epoch [64], Loss: 1.9166
Learning rate: 0.000726
Epoch [65], Loss: 1.9099
Learning rate: 0.000722
Epoch [66], Loss: 1.9041
Learning rate: 0.000718
Epoch [67], Loss: 1.8984
Learning rate: 0.000715
Epoch [68], Loss: 1.8929
Learning rate: 0.000711
Epoch [69], Loss: 1.8880
Learning rate: 0.000708
Epoch [70], Loss: 1.8813
Learning rate: 0.000704
Epoch [71], Loss: 1.8759
Learning rate: 0.000701
Epoch [72], Loss: 1.8715
Learning rate: 0.000697
Epoch [73], Loss: 1.8670
Learning rate: 0.000694
Epoch [74], Loss: 1.8613
Learning rate: 0.000690
Epoch [75], Loss: 1.8561
Learning rate: 0.000687
Epoch [76], Loss: 1.8511
Learning rate: 0.000683
Epoch [77], Loss: 1.8466
Learning rate: 0.000680
Epoch [78], Loss: 1.8406
Learning rate: 0.000676
Epoch [79], Loss: 1.8367
Learning rate: 0.000673
Epoch [80], Loss: 1.8297
Learning rate: 0.000670
Epoch [81], Loss: 1.8264
Learning rate: 0.000666
Epoch [82], Loss: 1.8210
Learning rate: 0.000663
Epoch [83], Loss: 1.8169
Learning rate: 0.000660
Epoch [84], Loss: 1.8119
Learning rate: 0.000656
Epoch [85], Loss: 1.8070
Learning rate: 0.000653
Epoch [86], Loss: 1.8024
Learning rate: 0.000650
Epoch [87], Loss: 1.7964
Learning rate: 0.000647
Epoch [88], Loss: 1.7950
Learning rate: 0.000643
Epoch [89], Loss: 1.7892
Learning rate: 0.000640
Epoch [90], Loss: 1.7861
Learning rate: 0.000637
Epoch [91], Loss: 1.7830
Learning rate: 0.000634
Epoch [92], Loss: 1.7768
Learning rate: 0.000631
Epoch [93], Loss: 1.7748
Learning rate: 0.000627
Epoch [94], Loss: 1.7696
Learning rate: 0.000624
Epoch [95], Loss: 1.7673
Learning rate: 0.000621
Epoch [96], Loss: 1.7623
Learning rate: 0.000618
Epoch [97], Loss: 1.7583
Learning rate: 0.000615
Epoch [98], Loss: 1.7545
Learning rate: 0.000612
Epoch [99], Loss: 1.7489
Learning rate: 0.000609
Epoch [100], Loss: 1.7475
Learning rate: 0.000606
Epoch [101], Loss: 1.7411
Learning rate: 0.000603
Epoch [102], Loss: 1.7386
Learning rate: 0.000600
Randomizing 100% of the dataset
Train set size 60000
Validation set size 0
Test set size 10000
Epoch [1], Loss: 2.5120
Learning rate: 0.000995
Epoch [2], Loss: 2.3031
Learning rate: 0.000990
Epoch [3], Loss: 2.3020
Learning rate: 0.000985
Epoch [4], Loss: 2.3014
Learning rate: 0.000980
Epoch [5], Loss: 2.3011
Learning rate: 0.000975
Epoch [6], Loss: 2.3006
Learning rate: 0.000970
Epoch [7], Loss: 2.2998
Learning rate: 0.000966
Epoch [8], Loss: 2.2994
Learning rate: 0.000961
Epoch [9], Loss: 2.2989
Learning rate: 0.000956
Epoch [10], Loss: 2.2990
Learning rate: 0.000951
Epoch [11], Loss: 2.2982
Learning rate: 0.000946
Epoch [12], Loss: 2.2974
Learning rate: 0.000942
Epoch [13], Loss: 2.2968
Learning rate: 0.000937
Epoch [14], Loss: 2.2961
Learning rate: 0.000932
Epoch [15], Loss: 2.2958
Learning rate: 0.000928
Epoch [16], Loss: 2.2950
Learning rate: 0.000923
Epoch [17], Loss: 2.2938
Learning rate: 0.000918
Epoch [18], Loss: 2.2930
Learning rate: 0.000914
Epoch [19], Loss: 2.2922
Learning rate: 0.000909
Epoch [20], Loss: 2.2908
Learning rate: 0.000905
Epoch [21], Loss: 2.2900
Learning rate: 0.000900
Epoch [22], Loss: 2.2887
Learning rate: 0.000896
Epoch [23], Loss: 2.2878
Learning rate: 0.000891
Epoch [24], Loss: 2.2865
Learning rate: 0.000887
Epoch [25], Loss: 2.2849
Learning rate: 0.000882
Epoch [26], Loss: 2.2834
Learning rate: 0.000878
Epoch [27], Loss: 2.2816
Learning rate: 0.000873
Epoch [28], Loss: 2.2804
Learning rate: 0.000869
Epoch [29], Loss: 2.2788
Learning rate: 0.000865
Epoch [30], Loss: 2.2770
Learning rate: 0.000860
Epoch [31], Loss: 2.2764
Learning rate: 0.000856
Epoch [32], Loss: 2.2750
Learning rate: 0.000852
Epoch [33], Loss: 2.2735
Learning rate: 0.000848
Epoch [34], Loss: 2.2722
Learning rate: 0.000843
Epoch [35], Loss: 2.2705
Learning rate: 0.000839
Epoch [36], Loss: 2.2684
Learning rate: 0.000835
Epoch [37], Loss: 2.2677
Learning rate: 0.000831
Epoch [38], Loss: 2.2663
Learning rate: 0.000827
Epoch [39], Loss: 2.2650
Learning rate: 0.000822
Epoch [40], Loss: 2.2634
Learning rate: 0.000818
Epoch [41], Loss: 2.2618
Learning rate: 0.000814
Epoch [42], Loss: 2.2601
Learning rate: 0.000810
Epoch [43], Loss: 2.2592
Learning rate: 0.000806
Epoch [44], Loss: 2.2564
Learning rate: 0.000802
Epoch [45], Loss: 2.2556
Learning rate: 0.000798
Epoch [46], Loss: 2.2537
Learning rate: 0.000794
Epoch [47], Loss: 2.2519
Learning rate: 0.000790
Epoch [48], Loss: 2.2496
Learning rate: 0.000786
Epoch [49], Loss: 2.2486
Learning rate: 0.000782
Epoch [50], Loss: 2.2468
Learning rate: 0.000778
Epoch [51], Loss: 2.2446
Learning rate: 0.000774
Epoch [52], Loss: 2.2427
Learning rate: 0.000771
Epoch [53], Loss: 2.2407
Learning rate: 0.000767
Epoch [54], Loss: 2.2384
Learning rate: 0.000763
Epoch [55], Loss: 2.2357
Learning rate: 0.000759
Epoch [56], Loss: 2.2344
Learning rate: 0.000755
Epoch [57], Loss: 2.2330
Learning rate: 0.000751
Epoch [58], Loss: 2.2313
Learning rate: 0.000748
Epoch [59], Loss: 2.2295
Learning rate: 0.000744
Epoch [60], Loss: 2.2277
Learning rate: 0.000740
Epoch [61], Loss: 2.2251
Learning rate: 0.000737
Epoch [62], Loss: 2.2242
Learning rate: 0.000733
Epoch [63], Loss: 2.2224
Learning rate: 0.000729
Epoch [64], Loss: 2.2203
Learning rate: 0.000726
Epoch [65], Loss: 2.2177
Learning rate: 0.000722
Epoch [66], Loss: 2.2159
Learning rate: 0.000718
Epoch [67], Loss: 2.2138
Learning rate: 0.000715
Epoch [68], Loss: 2.2111
Learning rate: 0.000711
Epoch [69], Loss: 2.2095
Learning rate: 0.000708
Epoch [70], Loss: 2.2073
Learning rate: 0.000704
Epoch [71], Loss: 2.2060
Learning rate: 0.000701
Epoch [72], Loss: 2.2028
Learning rate: 0.000697
Epoch [73], Loss: 2.2016
Learning rate: 0.000694
Epoch [74], Loss: 2.1987
Learning rate: 0.000690
Epoch [75], Loss: 2.1979
Learning rate: 0.000687
Epoch [76], Loss: 2.1964
Learning rate: 0.000683
Epoch [77], Loss: 2.1941
Learning rate: 0.000680
Epoch [78], Loss: 2.1929
Learning rate: 0.000676
Epoch [79], Loss: 2.1899
Learning rate: 0.000673
Epoch [80], Loss: 2.1886
Learning rate: 0.000670
Epoch [81], Loss: 2.1867
Learning rate: 0.000666
Epoch [82], Loss: 2.1851
Learning rate: 0.000663
Epoch [83], Loss: 2.1827
Learning rate: 0.000660
Epoch [84], Loss: 2.1809
Learning rate: 0.000656
Epoch [85], Loss: 2.1794
Learning rate: 0.000653
Epoch [86], Loss: 2.1776
Learning rate: 0.000650
Epoch [87], Loss: 2.1767
Learning rate: 0.000647
Epoch [88], Loss: 2.1739
Learning rate: 0.000643
Epoch [89], Loss: 2.1729
Learning rate: 0.000640
Epoch [90], Loss: 2.1703
Learning rate: 0.000637
Epoch [91], Loss: 2.1682
Learning rate: 0.000634
Epoch [92], Loss: 2.1660
Learning rate: 0.000631
Epoch [93], Loss: 2.1636
Learning rate: 0.000627
Epoch [94], Loss: 2.1613
Learning rate: 0.000624
Epoch [95], Loss: 2.1608
Learning rate: 0.000621
Epoch [96], Loss: 2.1593
Learning rate: 0.000618
Epoch [97], Loss: 2.1570
Learning rate: 0.000615
Epoch [98], Loss: 2.1549
Learning rate: 0.000612
Epoch [99], Loss: 2.1533
Learning rate: 0.000609
Epoch [100], Loss: 2.1523
Learning rate: 0.000606
Epoch [101], Loss: 2.1498
Learning rate: 0.000603
Epoch [102], Loss: 2.1474
Learning rate: 0.000600
