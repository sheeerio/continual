Randomizing 100% of the dataset
Train set size 60000
Validation set size 0
Test set size 10000
Epoch [1], Loss: 2.3072
Learning rate: 0.000995
Epoch [2], Loss: 2.2989
Learning rate: 0.000990
Epoch [3], Loss: 2.2934
Learning rate: 0.000985
Epoch [4], Loss: 2.2839
Learning rate: 0.000980
Epoch [5], Loss: 2.2683
Learning rate: 0.000975
Epoch [6], Loss: 2.2468
Learning rate: 0.000970
Epoch [7], Loss: 2.2143
Learning rate: 0.000966
Epoch [8], Loss: 2.1715
Learning rate: 0.000961
Epoch [9], Loss: 2.1170
Learning rate: 0.000956
Epoch [10], Loss: 2.0502
Learning rate: 0.000951
Epoch [11], Loss: 1.9751
Learning rate: 0.000946
Epoch [12], Loss: 1.8991
Learning rate: 0.000942
Epoch [13], Loss: 1.8097
Learning rate: 0.000937
Epoch [14], Loss: 1.7217
Learning rate: 0.000932
Epoch [15], Loss: 1.6337
Learning rate: 0.000928
Epoch [16], Loss: 1.5444
Learning rate: 0.000923
Epoch [17], Loss: 1.4618
Learning rate: 0.000918
Epoch [18], Loss: 1.3745
Learning rate: 0.000914
Epoch [19], Loss: 1.2991
Learning rate: 0.000909
Epoch [20], Loss: 1.2178
Learning rate: 0.000905
Epoch [21], Loss: 1.1383
Learning rate: 0.000900
Epoch [22], Loss: 1.0645
Learning rate: 0.000896
Epoch [23], Loss: 0.9981
Learning rate: 0.000891
Epoch [24], Loss: 0.9337
Learning rate: 0.000887
Epoch [25], Loss: 0.8756
Learning rate: 0.000882
Epoch [26], Loss: 0.8133
Learning rate: 0.000878
Epoch [27], Loss: 0.7659
Learning rate: 0.000873
Epoch [28], Loss: 0.7139
Learning rate: 0.000869
Epoch [29], Loss: 0.6627
Learning rate: 0.000865
Epoch [30], Loss: 0.6220
Learning rate: 0.000860
Epoch [31], Loss: 0.5784
Learning rate: 0.000856
Epoch [32], Loss: 0.5425
Learning rate: 0.000852
Epoch [33], Loss: 0.5070
Learning rate: 0.000848
Epoch [34], Loss: 0.4695
Learning rate: 0.000843
Epoch [35], Loss: 0.4416
Learning rate: 0.000839
Epoch [36], Loss: 0.4156
Learning rate: 0.000835
Epoch [37], Loss: 0.3848
Learning rate: 0.000831
Epoch [38], Loss: 0.3571
Learning rate: 0.000827
Epoch [39], Loss: 0.3330
Learning rate: 0.000822
Epoch [40], Loss: 0.3159
Learning rate: 0.000818
Epoch [41], Loss: 0.2982
Learning rate: 0.000814
Epoch [42], Loss: 0.2806
Learning rate: 0.000810
Epoch [43], Loss: 0.2652
Learning rate: 0.000806
Epoch [44], Loss: 0.2477
Learning rate: 0.000802
Epoch [45], Loss: 0.2335
Learning rate: 0.000798
Epoch [46], Loss: 0.2197
Learning rate: 0.000794
Epoch [47], Loss: 0.2039
Learning rate: 0.000790
Epoch [48], Loss: 0.1950
Learning rate: 0.000786
Target loss reached. Stopping training.
Randomizing 100% of the dataset
Train set size 60000
Validation set size 0
Test set size 10000
Epoch [1], Loss: 2.6683
Learning rate: 0.000995
Epoch [2], Loss: 2.3022
Learning rate: 0.000990
Epoch [3], Loss: 2.3018
Learning rate: 0.000985
Epoch [4], Loss: 2.3013
Learning rate: 0.000980
Epoch [5], Loss: 2.3006
Learning rate: 0.000975
Epoch [6], Loss: 2.3001
Learning rate: 0.000970
Epoch [7], Loss: 2.2994
Learning rate: 0.000966
Epoch [8], Loss: 2.2984
Learning rate: 0.000961
Epoch [9], Loss: 2.2974
Learning rate: 0.000956
Epoch [10], Loss: 2.2956
Learning rate: 0.000951
Epoch [11], Loss: 2.2934
Learning rate: 0.000946
Epoch [12], Loss: 2.2917
Learning rate: 0.000942
Epoch [13], Loss: 2.2888
Learning rate: 0.000937
Epoch [14], Loss: 2.2860
Learning rate: 0.000932
Epoch [15], Loss: 2.2815
Learning rate: 0.000928
Epoch [16], Loss: 2.2770
Learning rate: 0.000923
Epoch [17], Loss: 2.2715
Learning rate: 0.000918
Epoch [18], Loss: 2.2646
Learning rate: 0.000914
Epoch [19], Loss: 2.2565
Learning rate: 0.000909
Epoch [20], Loss: 2.2479
Learning rate: 0.000905
Epoch [21], Loss: 2.2376
Learning rate: 0.000900
Epoch [22], Loss: 2.2250
Learning rate: 0.000896
Epoch [23], Loss: 2.2118
Learning rate: 0.000891
Epoch [24], Loss: 2.1971
Learning rate: 0.000887
Epoch [25], Loss: 2.1798
Learning rate: 0.000882
Epoch [26], Loss: 2.1621
Learning rate: 0.000878
Epoch [27], Loss: 2.1409
Learning rate: 0.000873
Epoch [28], Loss: 2.1185
Learning rate: 0.000869
Epoch [29], Loss: 2.0948
Learning rate: 0.000865
Epoch [30], Loss: 2.0697
Learning rate: 0.000860
Epoch [31], Loss: 2.0430
Learning rate: 0.000856
Epoch [32], Loss: 2.0136
Learning rate: 0.000852
Epoch [33], Loss: 1.9829
Learning rate: 0.000848
Epoch [34], Loss: 1.9534
Learning rate: 0.000843
Epoch [35], Loss: 1.9217
Learning rate: 0.000839
Epoch [36], Loss: 1.8877
Learning rate: 0.000835
Epoch [37], Loss: 1.8547
Learning rate: 0.000831
Epoch [38], Loss: 1.8185
Learning rate: 0.000827
Epoch [39], Loss: 1.7877
Learning rate: 0.000822
Epoch [40], Loss: 1.7486
Learning rate: 0.000818
Epoch [41], Loss: 1.7151
Learning rate: 0.000814
Epoch [42], Loss: 1.6812
Learning rate: 0.000810
Epoch [43], Loss: 1.6447
Learning rate: 0.000806
Epoch [44], Loss: 1.6117
Learning rate: 0.000802
Epoch [45], Loss: 1.5775
Learning rate: 0.000798
Epoch [46], Loss: 1.5419
Learning rate: 0.000794
Epoch [47], Loss: 1.5094
Learning rate: 0.000790
Epoch [48], Loss: 1.4757
Learning rate: 0.000786
Epoch [49], Loss: 1.4431
Learning rate: 0.000782
Epoch [50], Loss: 1.4134
Learning rate: 0.000778
Epoch [51], Loss: 1.3817
Learning rate: 0.000774
Epoch [52], Loss: 1.3529
Learning rate: 0.000771
Epoch [53], Loss: 1.3208
Learning rate: 0.000767
Epoch [54], Loss: 1.2928
Learning rate: 0.000763
Epoch [55], Loss: 1.2638
Learning rate: 0.000759
Epoch [56], Loss: 1.2344
Learning rate: 0.000755
Epoch [57], Loss: 1.2059
Learning rate: 0.000751
Epoch [58], Loss: 1.1805
Learning rate: 0.000748
Epoch [59], Loss: 1.1531
Learning rate: 0.000744
Epoch [60], Loss: 1.1287
Learning rate: 0.000740
Epoch [61], Loss: 1.1024
Learning rate: 0.000737
Epoch [62], Loss: 1.0782
Learning rate: 0.000733
Epoch [63], Loss: 1.0564
Learning rate: 0.000729
Epoch [64], Loss: 1.0326
Learning rate: 0.000726
Epoch [65], Loss: 1.0106
Learning rate: 0.000722
Epoch [66], Loss: 0.9883
Learning rate: 0.000718
Epoch [67], Loss: 0.9675
Learning rate: 0.000715
Epoch [68], Loss: 0.9464
Learning rate: 0.000711
Epoch [69], Loss: 0.9256
Learning rate: 0.000708
Epoch [70], Loss: 0.9076
Learning rate: 0.000704
Epoch [71], Loss: 0.8881
Learning rate: 0.000701
Epoch [72], Loss: 0.8705
Learning rate: 0.000697
Epoch [73], Loss: 0.8511
Learning rate: 0.000694
Epoch [74], Loss: 0.8328
Learning rate: 0.000690
Epoch [75], Loss: 0.8167
Learning rate: 0.000687
Epoch [76], Loss: 0.8000
Learning rate: 0.000683
Epoch [77], Loss: 0.7829
Learning rate: 0.000680
Epoch [78], Loss: 0.7689
Learning rate: 0.000676
Epoch [79], Loss: 0.7525
Learning rate: 0.000673
Epoch [80], Loss: 0.7371
Learning rate: 0.000670
Epoch [81], Loss: 0.7220
Learning rate: 0.000666
Epoch [82], Loss: 0.7085
Learning rate: 0.000663
Epoch [83], Loss: 0.7012
Learning rate: 0.000660
Epoch [84], Loss: 0.6829
Learning rate: 0.000656
Epoch [85], Loss: 0.6727
Learning rate: 0.000653
Epoch [86], Loss: 0.6559
Learning rate: 0.000650
Epoch [87], Loss: 0.6431
Learning rate: 0.000647
Epoch [88], Loss: 0.6284
Learning rate: 0.000643
Epoch [89], Loss: 0.6189
Learning rate: 0.000640
Epoch [90], Loss: 0.6090
Learning rate: 0.000637
Epoch [91], Loss: 0.5942
Learning rate: 0.000634
Epoch [92], Loss: 0.5806
Learning rate: 0.000631
Epoch [93], Loss: 0.5722
Learning rate: 0.000627
Epoch [94], Loss: 0.5609
Learning rate: 0.000624
Epoch [95], Loss: 0.5536
Learning rate: 0.000621
Epoch [96], Loss: 0.5442
Learning rate: 0.000618
Epoch [97], Loss: 0.5358
Learning rate: 0.000615
Epoch [98], Loss: 0.5291
Learning rate: 0.000612
Epoch [99], Loss: 0.5188
Learning rate: 0.000609
Epoch [100], Loss: 0.5081
Learning rate: 0.000606
Epoch [101], Loss: 0.4974
Learning rate: 0.000603
Epoch [102], Loss: 0.4854
Learning rate: 0.000600
Randomizing 100% of the dataset
Train set size 60000
Validation set size 0
Test set size 10000
Epoch [1], Loss: 2.5158
Learning rate: 0.000995
Epoch [2], Loss: 2.3024
Learning rate: 0.000990
Epoch [3], Loss: 2.3022
Learning rate: 0.000985
Epoch [4], Loss: 2.3022
Learning rate: 0.000980
Epoch [5], Loss: 2.3021
Learning rate: 0.000975
Epoch [6], Loss: 2.3020
Learning rate: 0.000970
Epoch [7], Loss: 2.3019
Learning rate: 0.000966
Epoch [8], Loss: 2.3019
Learning rate: 0.000961
Epoch [9], Loss: 2.3020
Learning rate: 0.000956
Epoch [10], Loss: 2.3018
Learning rate: 0.000951
Epoch [11], Loss: 2.3018
Learning rate: 0.000946
Epoch [12], Loss: 2.3016
Learning rate: 0.000942
Epoch [13], Loss: 2.3016
Learning rate: 0.000937
Epoch [14], Loss: 2.3014
Learning rate: 0.000932
Epoch [15], Loss: 2.3018
Learning rate: 0.000928
Epoch [16], Loss: 2.3016
Learning rate: 0.000923
Epoch [17], Loss: 2.3012
Learning rate: 0.000918
Epoch [18], Loss: 2.3011
Learning rate: 0.000914
Epoch [19], Loss: 2.3009
Learning rate: 0.000909
Epoch [20], Loss: 2.3009
Learning rate: 0.000905
Epoch [21], Loss: 2.3010
Learning rate: 0.000900
Epoch [22], Loss: 2.3006
Learning rate: 0.000896
Epoch [23], Loss: 2.3003
Learning rate: 0.000891
Epoch [24], Loss: 2.3000
Learning rate: 0.000887
Epoch [25], Loss: 2.2998
Learning rate: 0.000882
Epoch [26], Loss: 2.2995
Learning rate: 0.000878
Epoch [27], Loss: 2.2993
Learning rate: 0.000873
Epoch [28], Loss: 2.2989
Learning rate: 0.000869
Epoch [29], Loss: 2.2987
Learning rate: 0.000865
Epoch [30], Loss: 2.2985
Learning rate: 0.000860
Epoch [31], Loss: 2.2985
Learning rate: 0.000856
Epoch [32], Loss: 2.2978
Learning rate: 0.000852
Epoch [33], Loss: 2.2979
Learning rate: 0.000848
Epoch [34], Loss: 2.2973
Learning rate: 0.000843
Epoch [35], Loss: 2.2974
Learning rate: 0.000839
Epoch [36], Loss: 2.2970
Learning rate: 0.000835
Epoch [37], Loss: 2.2968
Learning rate: 0.000831
Epoch [38], Loss: 2.2969
Learning rate: 0.000827
Epoch [39], Loss: 2.2966
Learning rate: 0.000822
Epoch [40], Loss: 2.2968
Learning rate: 0.000818
Epoch [41], Loss: 2.2958
Learning rate: 0.000814
Epoch [42], Loss: 2.2960
Learning rate: 0.000810
Epoch [43], Loss: 2.2953
Learning rate: 0.000806
Epoch [44], Loss: 2.2949
Learning rate: 0.000802
Epoch [45], Loss: 2.2950
Learning rate: 0.000798
Epoch [46], Loss: 2.2944
Learning rate: 0.000794
Epoch [47], Loss: 2.2929
Learning rate: 0.000790
Epoch [48], Loss: 2.2929
Learning rate: 0.000786
Epoch [49], Loss: 2.2923
Learning rate: 0.000782
Epoch [50], Loss: 2.2912
Learning rate: 0.000778
Epoch [51], Loss: 2.2908
Learning rate: 0.000774
Epoch [52], Loss: 2.2905
Learning rate: 0.000771
Epoch [53], Loss: 2.2899
Learning rate: 0.000767
Epoch [54], Loss: 2.2892
Learning rate: 0.000763
Epoch [55], Loss: 2.2883
Learning rate: 0.000759
Epoch [56], Loss: 2.2878
Learning rate: 0.000755
Epoch [57], Loss: 2.2876
Learning rate: 0.000751
Epoch [58], Loss: 2.2866
Learning rate: 0.000748
Epoch [59], Loss: 2.2857
Learning rate: 0.000744
Epoch [60], Loss: 2.2848
Learning rate: 0.000740
Epoch [61], Loss: 2.2846
Learning rate: 0.000737
Epoch [62], Loss: 2.2836
Learning rate: 0.000733
Epoch [63], Loss: 2.2826
Learning rate: 0.000729
Epoch [64], Loss: 2.2817
Learning rate: 0.000726
Epoch [65], Loss: 2.2821
Learning rate: 0.000722
Epoch [66], Loss: 2.2812
Learning rate: 0.000718
Epoch [67], Loss: 2.2809
Learning rate: 0.000715
Epoch [68], Loss: 2.2792
Learning rate: 0.000711
Epoch [69], Loss: 2.2775
Learning rate: 0.000708
Epoch [70], Loss: 2.2767
Learning rate: 0.000704
Epoch [71], Loss: 2.2754
Learning rate: 0.000701
Epoch [72], Loss: 2.2740
Learning rate: 0.000697
Epoch [73], Loss: 2.2721
Learning rate: 0.000694
Epoch [74], Loss: 2.2718
Learning rate: 0.000690
Epoch [75], Loss: 2.2714
Learning rate: 0.000687
Epoch [76], Loss: 2.2699
Learning rate: 0.000683
Epoch [77], Loss: 2.2692
Learning rate: 0.000680
Epoch [78], Loss: 2.2677
Learning rate: 0.000676
Epoch [79], Loss: 2.2677
Learning rate: 0.000673
Epoch [80], Loss: 2.2662
Learning rate: 0.000670
Epoch [81], Loss: 2.2650
Learning rate: 0.000666
Epoch [82], Loss: 2.2629
Learning rate: 0.000663
Epoch [83], Loss: 2.2620
Learning rate: 0.000660
Epoch [84], Loss: 2.2603
Learning rate: 0.000656
Epoch [85], Loss: 2.2596
Learning rate: 0.000653
Epoch [86], Loss: 2.2580
Learning rate: 0.000650
Epoch [87], Loss: 2.2563
Learning rate: 0.000647
Epoch [88], Loss: 2.2555
Learning rate: 0.000643
Epoch [89], Loss: 2.2530
Learning rate: 0.000640
Epoch [90], Loss: 2.2528
Learning rate: 0.000637
Epoch [91], Loss: 2.2505
Learning rate: 0.000634
Epoch [92], Loss: 2.2493
Learning rate: 0.000631
Epoch [93], Loss: 2.2484
Learning rate: 0.000627
Epoch [94], Loss: 2.2467
Learning rate: 0.000624
Epoch [95], Loss: 2.2459
Learning rate: 0.000621
Epoch [96], Loss: 2.2451
Learning rate: 0.000618
Epoch [97], Loss: 2.2424
Learning rate: 0.000615
Epoch [98], Loss: 2.2414
Learning rate: 0.000612
Epoch [99], Loss: 2.2403
Learning rate: 0.000609
Epoch [100], Loss: 2.2391
Learning rate: 0.000606
Epoch [101], Loss: 2.2376
Learning rate: 0.000603
Epoch [102], Loss: 2.2360
Learning rate: 0.000600
Randomizing 100% of the dataset
Train set size 60000
Validation set size 0
Test set size 10000
Epoch [1], Loss: 2.3196
Learning rate: 0.000995
Epoch [2], Loss: 2.3024
Learning rate: 0.000990
Epoch [3], Loss: 2.3022
Learning rate: 0.000985
Epoch [4], Loss: 2.3022
Learning rate: 0.000980
Epoch [5], Loss: 2.3022
Learning rate: 0.000975
Epoch [6], Loss: 2.3021
Learning rate: 0.000970
Epoch [7], Loss: 2.3021
Learning rate: 0.000966
Epoch [8], Loss: 2.3020
Learning rate: 0.000961
Epoch [9], Loss: 2.3020
Learning rate: 0.000956
Epoch [10], Loss: 2.3020
Learning rate: 0.000951
Epoch [11], Loss: 2.3019
Learning rate: 0.000946
Epoch [12], Loss: 2.3019
Learning rate: 0.000942
Epoch [13], Loss: 2.3020
Learning rate: 0.000937
Epoch [14], Loss: 2.3018
Learning rate: 0.000932
Epoch [15], Loss: 2.3017
Learning rate: 0.000928
Epoch [16], Loss: 2.3018
Learning rate: 0.000923
Epoch [17], Loss: 2.3018
Learning rate: 0.000918
Epoch [18], Loss: 2.3018
Learning rate: 0.000914
Epoch [19], Loss: 2.3019
Learning rate: 0.000909
Epoch [20], Loss: 2.3018
Learning rate: 0.000905
Epoch [21], Loss: 2.3017
Learning rate: 0.000900
Epoch [22], Loss: 2.3017
Learning rate: 0.000896
Epoch [23], Loss: 2.3018
Learning rate: 0.000891
Epoch [24], Loss: 2.3018
Learning rate: 0.000887
Epoch [25], Loss: 2.3019
Learning rate: 0.000882
Epoch [26], Loss: 2.3017
Learning rate: 0.000878
Epoch [27], Loss: 2.3018
Learning rate: 0.000873
Epoch [28], Loss: 2.3018
Learning rate: 0.000869
Epoch [29], Loss: 2.3017
Learning rate: 0.000865
Epoch [30], Loss: 2.3017
Learning rate: 0.000860
Epoch [31], Loss: 2.3016
Learning rate: 0.000856
Epoch [32], Loss: 2.3017
Learning rate: 0.000852
Epoch [33], Loss: 2.3017
Learning rate: 0.000848
Epoch [34], Loss: 2.3019
Learning rate: 0.000843
Epoch [35], Loss: 2.3016
Learning rate: 0.000839
Epoch [36], Loss: 2.3017
Learning rate: 0.000835
Epoch [37], Loss: 2.3017
Learning rate: 0.000831
Epoch [38], Loss: 2.3016
Learning rate: 0.000827
Epoch [39], Loss: 2.3016
Learning rate: 0.000822
Epoch [40], Loss: 2.3016
Learning rate: 0.000818
Epoch [41], Loss: 2.3015
Learning rate: 0.000814
Epoch [42], Loss: 2.3016
Learning rate: 0.000810
Epoch [43], Loss: 2.3018
Learning rate: 0.000806
Epoch [44], Loss: 2.3017
Learning rate: 0.000802
Epoch [45], Loss: 2.3018
Learning rate: 0.000798
Epoch [46], Loss: 2.3018
Learning rate: 0.000794
Epoch [47], Loss: 2.3020
Learning rate: 0.000790
Epoch [48], Loss: 2.3019
Learning rate: 0.000786
Epoch [49], Loss: 2.3018
Learning rate: 0.000782
Epoch [50], Loss: 2.3019
Learning rate: 0.000778
Epoch [51], Loss: 2.3018
Learning rate: 0.000774
Epoch [52], Loss: 2.3018
Learning rate: 0.000771
Epoch [53], Loss: 2.3018
Learning rate: 0.000767
Epoch [54], Loss: 2.3018
Learning rate: 0.000763
Epoch [55], Loss: 2.3018
Learning rate: 0.000759
Epoch [56], Loss: 2.3018
Learning rate: 0.000755
Epoch [57], Loss: 2.3021
Learning rate: 0.000751
Epoch [58], Loss: 2.3020
Learning rate: 0.000748
Epoch [59], Loss: 2.3021
Learning rate: 0.000744
Epoch [60], Loss: 2.3020
Learning rate: 0.000740
Epoch [61], Loss: 2.3019
Learning rate: 0.000737
Epoch [62], Loss: 2.3019
Learning rate: 0.000733
Epoch [63], Loss: 2.3019
Learning rate: 0.000729
Epoch [64], Loss: 2.3018
Learning rate: 0.000726
Epoch [65], Loss: 2.3019
Learning rate: 0.000722
Epoch [66], Loss: 2.3019
Learning rate: 0.000718
Epoch [67], Loss: 2.3019
Learning rate: 0.000715
Epoch [68], Loss: 2.3019
Learning rate: 0.000711
Epoch [69], Loss: 2.3019
Learning rate: 0.000708
Epoch [70], Loss: 2.3019
Learning rate: 0.000704
Epoch [71], Loss: 2.3020
Learning rate: 0.000701
Epoch [72], Loss: 2.3019
Learning rate: 0.000697
Epoch [73], Loss: 2.3021
Learning rate: 0.000694
Epoch [74], Loss: 2.3019
Learning rate: 0.000690
Epoch [75], Loss: 2.3019
Learning rate: 0.000687
Epoch [76], Loss: 2.3019
Learning rate: 0.000683
Epoch [77], Loss: 2.3019
Learning rate: 0.000680
Epoch [78], Loss: 2.3019
Learning rate: 0.000676
Epoch [79], Loss: 2.3020
Learning rate: 0.000673
Epoch [80], Loss: 2.3019
Learning rate: 0.000670
Epoch [81], Loss: 2.3020
Learning rate: 0.000666
Epoch [82], Loss: 2.3020
Learning rate: 0.000663
Epoch [83], Loss: 2.3019
Learning rate: 0.000660
Epoch [84], Loss: 2.3020
Learning rate: 0.000656
Epoch [85], Loss: 2.3019
Learning rate: 0.000653
Epoch [86], Loss: 2.3019
Learning rate: 0.000650
Epoch [87], Loss: 2.3019
Learning rate: 0.000647
Epoch [88], Loss: 2.3019
Learning rate: 0.000643
Epoch [89], Loss: 2.3023
Learning rate: 0.000640
Epoch [90], Loss: 2.3019
Learning rate: 0.000637
Epoch [91], Loss: 2.3018
Learning rate: 0.000634
Epoch [92], Loss: 2.3018
Learning rate: 0.000631
Epoch [93], Loss: 2.3018
Learning rate: 0.000627
Epoch [94], Loss: 2.3018
Learning rate: 0.000624
Epoch [95], Loss: 2.3019
Learning rate: 0.000621
Epoch [96], Loss: 2.3019
Learning rate: 0.000618
Epoch [97], Loss: 2.3020
Learning rate: 0.000615
Epoch [98], Loss: 2.3019
Learning rate: 0.000612
Epoch [99], Loss: 2.3019
Learning rate: 0.000609
Epoch [100], Loss: 2.3018
Learning rate: 0.000606
Epoch [101], Loss: 2.3018
Learning rate: 0.000603
Epoch [102], Loss: 2.3018
Learning rate: 0.000600
