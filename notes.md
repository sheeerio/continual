- [X] run with kaiming uniform <- set up already
- [X] does dropout help?
- [X] classic kaiming again
- [X] run dropout + kaiming again.
- [X] do the adam EMA and eps change
- [ ] how to check for dead units?
    - [ ] log the norm of activations?
    - [ ] log the effective rank of last layer?
    - [ ] does reinitializing them help?
- [X] does leaky relu help?
    - results in exactly same output as dropout (why? : thinking ...)
    - why does kaiming + leaky relu help better than kaiming + dropout?
    - i thought leaky_relu ~= dropout? but then maybe they're acting different here as well?
- [ ] does layernorm help?
- [ ] parameter values set to first run's and then run with a fresh optimizer -> want to see how much is the effect of the optimizer in this and how much of the parameter values. 

- [ ] why is plasticity important? why is the ability to quickly learn multiple tasks important? okay, one reason is long sparse-reward tasks in RL but anything else? robots? how? why? anything else?
- [ ] why is kaiming initialization helping learn faster, especially at the start? it does also help in the second run, why so stagnant afterwards? do the cumulative effects end its effects from the start? it's just an 'initialization' after all.
- [ ] why is leaky_relu helping sustain the learning speed over time?
- [ ] why is dropout helping sustain the learning speed over time? and is it related to what leaky_relu does here? is it doing the same thing? leaky_relu is better here though.
- [ ] why does dropout + kaiming not work as good as leaky_relu + kaiming?